\chapter{Proposed Hybrid CNNâ€“Transformer Model}
\section{Architectural Overview}
% Teacher decoder prior; student encoder from measurements.
\section{Stage 1: CNN Autoencoder (Teacher Prior)}
\subsection{Encoder: Residual Blocks and Downsampling}
\subsection{Latent Space Design ($d_z{=}256$)}
\subsection{Decoder: Progressive Upsampling and Reconstruction}
\section{Stage 2: Spatially-Aware Measurement Embedding (Contribution)}
\subsection{Signal Branch for $\{\log A,\varphi\}$}
\subsection{Position Branch for $\{\mathbf{x}_\mathrm{src},\mathbf{x}_\mathrm{det}\}$}
\subsection{Fusion and Token Formation}
\section{Transformer Encoder}
\subsection{Attention, Depth, and Positional Handling}
\subsection{Regularisation and Capacity Control}
\section{Aggregation via Multi-Query Attention Pooling (Contribution)}
% 4 learnable queries; contrast with mean/global pooling; motivation and equations.
\section{Latent Alignment and Decoder Reuse}
% Student $\hat{\mathbf z}$ matched to teacher $\mathbf z$; decoder fixed by default.
\section{Inference Pathway and Computational Cost}