\contentsline {chapter}{Abstract}{ii}{}%
\contentsline {chapter}{Acknowledgements}{iii}{}%
\contentsline {chapter}{Abbreviations}{iv}{}%
\contentsline {chapter}{List of Figures}{viii}{}%
\contentsline {chapter}{List of Tables}{ix}{}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{}%
\contentsline {section}{\numberline {1.1}Background and Motivation}{1}{}%
\contentsline {section}{\numberline {1.2}Near-Infrared Diffuse Optical Tomography (NIR-DOT)}{1}{}%
\contentsline {section}{\numberline {1.3}Challenges in NIR-DOT Reconstruction}{1}{}%
\contentsline {section}{\numberline {1.4}Research Objectives and Contributions}{1}{}%
\contentsline {section}{\numberline {1.5}Dissertation Structure}{2}{}%
\contentsline {chapter}{\numberline {2}Literature Review}{3}{}%
\contentsline {section}{\numberline {2.1}Physics of NIR Light Propagation}{3}{}%
\contentsline {section}{\numberline {2.2}Classical NIR-DOT Reconstruction Methods}{3}{}%
\contentsline {section}{\numberline {2.3}Deep Learning for Medical Image Reconstruction}{3}{}%
\contentsline {section}{\numberline {2.4}CNN Approaches in NIR-DOT}{3}{}%
\contentsline {section}{\numberline {2.5}Transformers in Medical Imaging}{4}{}%
\contentsline {section}{\numberline {2.6}Hybrid Architectures and Multi-Stage Training}{4}{}%
\contentsline {section}{\numberline {2.7}Generalisation Challenges in DOT and Inverse Problems}{4}{}%
\contentsline {section}{\numberline {2.8}Research Gap and Opportunity}{4}{}%
\contentsline {chapter}{\numberline {3}Synthetic Phantom Data Generation}{5}{}%
\contentsline {section}{\numberline {3.1}Physics-Based Forward Modelling}{5}{}%
\contentsline {section}{\numberline {3.2}Geometric Phantom Construction}{5}{}%
\contentsline {section}{\numberline {3.3}Optical Property Assignment}{5}{}%
\contentsline {section}{\numberline {3.4}Surface Extraction and Probe Placement}{5}{}%
\contentsline {section}{\numberline {3.5}Noise Model}{6}{}%
\contentsline {section}{\numberline {3.6}Dataset Composition and Preprocessing}{6}{}%
\contentsline {chapter}{\numberline {4}Hybrid CNN-Transformer Architecture}{7}{}%
\contentsline {section}{\numberline {4.1}Architectural Overview}{7}{}%
\contentsline {section}{\numberline {4.2}Stage 1: CNN Autoencoder Design}{7}{}%
\contentsline {subsection}{\numberline {4.2.1}Encoder Architecture}{7}{}%
\contentsline {subsection}{\numberline {4.2.2}Decoder Architecture}{7}{}%
\contentsline {subsection}{\numberline {4.2.3}Residual Connections and Latent Space}{7}{}%
\contentsline {section}{\numberline {4.3}Stage 2: Measurement Embedding and Transformer}{8}{}%
\contentsline {subsection}{\numberline {4.3.1}Spatially-Aware Embedding}{8}{}%
\contentsline {subsection}{\numberline {4.3.2}Transformer Encoder Design}{8}{}%
\contentsline {subsection}{\numberline {4.3.3}Latent Alignment Strategy}{8}{}%
\contentsline {section}{\numberline {4.4}Integration Strategy}{8}{}%
\contentsline {chapter}{\numberline {5}Two-Stage Training Methodology}{9}{}%
\contentsline {section}{\numberline {5.1}Training Strategy Overview}{9}{}%
\contentsline {section}{\numberline {5.2}Stage 1: CNN Autoencoder Pre-Training}{9}{}%
\contentsline {subsection}{\numberline {5.2.1}Loss Function and Optimisation}{9}{}%
\contentsline {subsection}{\numberline {5.2.2}Hyperparameter and Setup}{9}{}%
\contentsline {section}{\numberline {5.3}Stage 2: Transformer Enhancement Training}{9}{}%
\contentsline {subsection}{\numberline {5.3.1}Frozen Decoder Approach}{10}{}%
\contentsline {subsection}{\numberline {5.3.2}Latent Alignment Objective}{10}{}%
\contentsline {subsection}{\numberline {5.3.3}Scheduler and Optimisation Strategy}{10}{}%
\contentsline {section}{\numberline {5.4}Data Augmentation and Undersampling Strategy}{10}{}%
\contentsline {section}{\numberline {5.5}Implementation Details}{10}{}%
\contentsline {chapter}{\numberline {6}Experimental Results and Analysis}{11}{}%
\contentsline {section}{\numberline {6.1}Experimental Setup}{11}{}%
\contentsline {subsection}{\numberline {6.1.1}Dataset Preparation}{11}{}%
\contentsline {subsection}{\numberline {6.1.2}Evaluation Metrics}{11}{}%
\contentsline {subsection}{\numberline {6.1.3}Baseline Methods}{11}{}%
\contentsline {section}{\numberline {6.2}Stage 1 Results: CNN Autoencoder Performance}{11}{}%
\contentsline {section}{\numberline {6.3}Stage 2 Results: Transformer Enhancement}{11}{}%
\contentsline {section}{\numberline {6.4}Comparative Analysis}{12}{}%
\contentsline {section}{\numberline {6.5}Visualisation and Interpretation}{12}{}%
\contentsline {chapter}{\numberline {7}Discussion}{13}{}%
\contentsline {section}{\numberline {7.1}Key Findings and Insights}{13}{}%
\contentsline {section}{\numberline {7.2}Clinical Implications of Generalisable DOT Models}{13}{}%
\contentsline {section}{\numberline {7.3}Limitations of Current Approach}{13}{}%
\contentsline {section}{\numberline {7.4}Computational Efficiency Considerations}{13}{}%
\contentsline {chapter}{\numberline {8}Conclusion and Future Work}{14}{}%
\contentsline {section}{\numberline {8.1}Summary of Contributions}{14}{}%
\contentsline {section}{\numberline {8.2}Future Research Directions}{14}{}%
\contentsline {section}{\numberline {8.3}Potential Clinical Applications}{14}{}%
\contentsline {chapter}{\numberline {A}Implementation Details}{16}{}%
\contentsline {chapter}{\numberline {B}Additional Experimental Results}{17}{}%
\contentsline {chapter}{\numberline {C}Mathematical Derivations}{18}{}%
