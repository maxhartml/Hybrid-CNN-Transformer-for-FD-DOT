# ğŸ§¬ NIR-DOT Reconstruction: Two-Stage CNN-Transformer with Latent Alignment

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> *Towards Generalisable Inverse Modelling in Near-Infrared Diffuse Optical Tomography*
> 
> **MSc AI/ML Dissertation Project** â€¢ University of Birmingham â€¢ 2025

## ğŸ¯ Project Overview

This project tackles the **severely ill-posed inverse problem** in Near-Infrared Diffuse Optical Tomography (NIR-DOT): reconstructing 3D tissue optical properties from sparse boundary measurements. Our solution combines the spatial expertise of CNNs with the sequence modeling power of transformers through a novel **teacher-student latent alignment** approach.

### ğŸš€ Key Innovation
- **Two-stage hybrid architecture**: CNN spatial priors + Transformer sequence modeling
- **Latent alignment strategy**: Student transformer learns to match teacher CNN representations
- **Preserved decoder quality**: Frozen CNN decoder maintains spatial reconstruction fidelity
- **Clinical realism**: Physics-based phantom generation with realistic probe geometries

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STAGE 1: CNN AUTOENCODER                   â”‚
â”‚                                                                 â”‚
â”‚  Ground Truth â†’ 3D CNN Encoder â†’ [256D Latent] â†’ 3D CNN        â”‚
â”‚  Volumes                             â”‚           Decoder       â”‚
â”‚                                      â–¼              â”‚          â”‚
â”‚                                Teacher Latent       â–¼          â”‚
â”‚                                                Reconstructed    â”‚
â”‚                                                Ground Truth     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼ (Knowledge Transfer)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                STAGE 2: TRANSFORMER ENHANCEMENT                â”‚
â”‚                                                                 â”‚
â”‚  NIR Measurements â†’ Spatially-Aware â†’ Transformer â†’ Global     â”‚
â”‚                     Embedding         Encoder      Pooling     â”‚
â”‚                                                        â”‚        â”‚
â”‚                                                        â–¼        â”‚
â”‚                                                [256D Student    â”‚
â”‚                                                 Latent] â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚        â”‚
â”‚                         LATENT ALIGNMENT LOSS â†â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                         (Teacher â†” Student)                    â”‚
â”‚                                                                 â”‚
â”‚                                                        â–¼        â”‚
â”‚                                              Frozen CNN Decoder â”‚
â”‚                                                        â”‚        â”‚
â”‚                                                        â–¼        â”‚
â”‚                                              Enhanced Reconstruction
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key: â†’ Forward pass  â”€â”€â”€ Latent alignment  â–¼ Flow direction
```

### ğŸ”§ Component Breakdown

| Component | Purpose | Architecture |
|-----------|---------|--------------|
| **CNN Autoencoder** | Spatial feature learning | 16â†’32â†’64â†’128â†’256 channels, ResNet blocks |
| **Spatially-Aware Embedding** | Measurement tokenization | Separate MLPs for measurements + positions |
| **Transformer Encoder** | Sequence modeling | 8 layers Ã— 8 heads, d_model=256 |
| **Global Pooling** | Sequence aggregation | Multi-query attention (4 queries) |
| **Latent Alignment** | Knowledge transfer | RMSE loss between teacher-student |

---

## ğŸ“Š Data Pipeline

### Phantom Generation
- **ğŸ² Synthetic phantoms**: 64Â³ voxel volumes (1mm resolution)
- **ğŸ”¬ Realistic tissue properties**: Î¼â‚ âˆˆ [0.003, 0.007] mmâ»Â¹, Î¼'â‚› âˆˆ [0.78, 1.18] mmâ»Â¹
- **ğŸ©» Tumor inclusions**: 0-5 tumors per phantom, 5-15mm radius
- **ğŸŒ€ Random orientations**: Eliminate directional bias through 3D rotations

### Forward Modeling
- **âš¡ NIRFASTer-FF**: Frequency-domain FEM solver (140MHz)
- **ğŸ“¡ Realistic probe geometry**: 50 sources Ã— 20 detectors = 1000 measurements
- **ğŸ¯ Clinical SDS range**: 10-40mm source-detector separations
- **ğŸ”Š Realistic noise**: 0.5% amplitude + Â±0.5Â° phase noise

### Data Augmentation
- **ğŸ° Measurement subsampling**: 256 from 1000 measurements per training iteration
- **ğŸ“ˆ Dataset efficiency**: 3.9Ã— more training combinations per phantom
- **ğŸ”€ Randomized selection**: Different measurement subsets each epoch

---

## ğŸš€ Training Strategy

### Stage 1: CNN Foundation (200 epochs)
```python
# Spatial feature learning with aggressive optimization
optimizer = AdamW(lr=1e-4, betas=[0.9, 0.98], weight_decay=1e-3)
scheduler = OneCycleLR(max_lr=2e-3, pct_start=0.4, div_factor=20)
loss = RMSE(predictions, standardized_ground_truth)
```

**ğŸ¯ Goal**: Learn robust spatial representations and basic reconstruction capabilities

### Stage 2: Transformer Enhancement (400 epochs)
```python
# Teacher-student latent alignment
teacher_latent = frozen_cnn_encoder(ground_truth)  # 256D
student_latent = transformer_pipeline(nir_measurements)  # 256D
loss = RMSE(student_latent, teacher_latent)  # Latent-only training
```

**ğŸ¯ Goal**: Enhance CNN features with transformer sequence modeling while preserving decoder quality

---

## ğŸ“ Project Structure

```
mah422/
â”œâ”€â”€ ğŸ“‚ code/                    # Core implementation
â”‚   â”œâ”€â”€ ğŸ”¬ data_processing/     # Phantom generation & data loading
â”‚   â”œâ”€â”€ ğŸ¤– models/              # CNN, Transformer, Hybrid architectures
â”‚   â”œâ”€â”€ ğŸ‹ï¸ training/            # Stage 1 & 2 trainers + configs
â”‚   â””â”€â”€ ğŸ› ï¸ utils/               # Metrics, visualization, logging
â”œâ”€â”€ ğŸ“Š data/                   # Generated phantom datasets (HDF5)
â”œâ”€â”€ ğŸ’¾ checkpoints/            # Saved model states
â”œâ”€â”€ ğŸ“ˆ logs/                   # Training logs & metrics
â”œâ”€â”€ ğŸ¨ analysis_results/       # Visualization outputs
â””â”€â”€ ğŸ”§ nirfaster-FF/           # Forward modeling library
```

---

## âš¡ Quick Start

### 1. Environment Setup
```bash
# Create Python virtual environment
python3 -m venv nir-dot-env
source nir-dot-env/bin/activate  # On Windows: nir-dot-env\Scripts\activate

# Install dependencies from requirements
pip install -r requirements.txt
```

### 2. Generate Data
```bash
# Create synthetic phantom dataset
python -m code.data_processing.data_simulator
# Generates 5000 phantoms with realistic tissue properties
```

### 3. Train Models
```bash
# Stage 1: CNN Autoencoder pre-training
python -m code.training.train_hybrid_model  # CURRENT_TRAINING_STAGE = "stage1"

# Stage 2: Transformer enhancement
python -m code.training.train_hybrid_model  # CURRENT_TRAINING_STAGE = "stage2"
```

### 4. Analyze Results
```bash
# Dataset analysis
python -m code.data_processing.phantom_dataset_analysis

# Training visualization (Weights & Biases integration)
wandb login  # View training metrics at wandb.ai
```

---

## ğŸ›ï¸ Key Hyperparameters

| Stage | Optimizer | LR Schedule | Batch Size | Loss | Special Features |
|-------|-----------|-------------|------------|------|------------------|
| **1** | AdamW (1e-4) | OneCycleLR (max 2e-3) | 128 | RMSE | Momentum cycling, AMP |
| **2** | AdamW (3e-4) | Linear + Cosine | 128 | Latent RMSE | EMA, Frozen decoder |

---

## ğŸ“Š Results Highlights

### Reconstruction Quality
- **âœ… Stage 1**: Robust spatial features, sharp Î¼â‚ recovery
- **ğŸš€ Stage 2**: Enhanced global coherence, reduced artifacts
- **âš¡ Performance**: 2Ã— speedup with mixed precision training
- **ğŸ¯ Stability**: EMA provides improved generalization

### Technical Achievements
- **ğŸ§  Parameter efficiency**: 7M CNN parameters (optimized from 26.9M)
- **ğŸ’¾ Memory optimization**: 97% reduction in Stage 1 data loading
- **âš™ï¸ Infrastructure**: PyTorch 2.0 compilation, A100 optimization
- **ğŸ”¬ Physics compliance**: Realistic forward modeling with NIRFASTer-FF

---

## ğŸ› ï¸ Technical Features

### ğŸ¯ Architecture Innovations
- **Spatially-aware embedding**: Separate processing of measurements and coordinates
- **Multi-query attention pooling**: Efficient sequence aggregation
- **Frozen decoder preservation**: Maintains CNN spatial priors
- **Progressive EMA decay**: Stabilizes late-stage training

### âš¡ Engineering Excellence
- **Mixed precision training**: A100-optimized with conservative scaling
- **Deterministic evaluation**: Fixed seeds for reproducible results
- **Comprehensive logging**: W&B integration with detailed metrics
- **Memory-efficient data loading**: Stage-specific optimizations

### ğŸ”¬ Physics-Based Validation
- **Realistic tissue properties**: Clinically relevant optical coefficients  
- **Proper noise modeling**: SNR ~46dB matching clinical systems
- **SDS constraints**: 10-40mm range ensures diffusive regime
- **Surface-aware probe placement**: Morphologically extracted tissue boundaries

---

## ğŸ“ Research Context

This work addresses fundamental challenges in medical imaging inverse problems:

1. **ğŸ¯ Ill-posed inverse problem**: Sparse measurements â†’ dense 3D reconstructions
2. **ğŸ¥ Clinical translation gap**: Synthetic training â†’ real measurement generalization  
3. **âš–ï¸ Physics vs. learning trade-off**: Preserve domain knowledge while enabling ML enhancement
4. **ğŸ”§ Computational efficiency**: Real-time reconstruction for clinical workflows

Our **two-stage latent alignment** approach provides a principled framework for combining physics-based priors with data-driven enhancement.

---

## ğŸš§ Current Limitations & Future Work

### Limitations
- **ğŸ“Š Synthetic data only**: Requires domain adaptation for real clinical measurements
- **ğŸ” Î¼'â‚› recovery challenges**: Physics-limited scattering coefficient reconstruction
- **ğŸ’¾ Memory requirements**: 64Â³ volumes demand significant GPU memory
- **â±ï¸ Attention scaling**: Quadratic complexity limits sequence length

### Future Directions
- **ğŸ¥ Real data validation**: Clinical measurement integration with domain adaptation
- **ğŸŒˆ Multi-wavelength extension**: Spectroscopic DOT capabilities
- **ğŸ” Dynamic sequence undersampling**: Adaptive measurement selection
- **ğŸ§¬ Tissue-patch context**: Enhanced anatomical awareness (framework implemented)

---

## ğŸ“š Key References & Dependencies

### Core Libraries
- **ğŸ”¥ PyTorch 2.0+**: Deep learning framework with compilation
- **ğŸ”¬ NIRFASTer-FF**: Frequency-domain FEM forward modeling
- **ğŸ“Š Weights & Biases**: Experiment tracking and visualization
- **ğŸ’¾ HDF5**: Efficient dataset storage and loading

### Research Foundation
- Teacher-student knowledge distillation
- Transformer architectures for medical imaging
- Physics-informed neural networks
- Diffuse optical tomography reconstruction methods

---

## ğŸ“„ License & Citation

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

If you use this code in your research, please cite:
```bibtex
@mastersthesis{hart2025nirdot,
    title={Towards Generalisable Inverse Modelling in NIR-DOT: Two-Stage CNN-Transformer with Latent Alignment},
    author={Hart, Max},
    year={2025},
    school={University of Birmingham},
    type={MSc Dissertation}
}
```

---

<div align="center">

**ğŸ§  Built with curiosity â€¢ ğŸ”¬ Powered by physics â€¢ ğŸš€ Enhanced by AI**

*University of Birmingham â€¢ MSc Artificial Intelligence & Machine Learning â€¢ 2025*

</div>
