<div align="center">

<img src="figs/phantom_001_probe_layout.png" alt="NIR-DOT Architecture" width="300"/>

<h3>ğŸ¯ Towards Generalisable Inverse Modelling for Frequency-Domain DOT<br/>via a Hybrid CNNâ€“Transformer</h3>

[![Python](https://img.shields.io/badge/Python-3.10%2B-3776ab?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](./LICENSE)
[![W&B](https://img.shields.io/badge/Weights%20%26%20Biases-Optional-ffbe00?style=for-the-badge&logo=weightsandbiases&logoColor=black)](https://wandb.ai/)

**Robust, layout-agnostic deep learning for diffuse optical tomography reconstruction**  
*Two-stage architecture: autoencoder spatial prior + transformer geometry mapping*

[ğŸš€ Quick Start](#-quickstart) â€¢ 
[ğŸ“– Documentation](#-documentation) â€¢ 
[ğŸ”¬ Research](#-research) â€¢ 
[ğŸ¤ Contributing](#-contributing)

---

</div>

## ğŸŒŸ Overview

> **Revolutionary approach to NIR-DOT reconstruction combining the spatial learning power of CNNs with the geometric awareness of transformers.**

This repository implements a **two-stage deep learning pipeline** for Near-Infrared Diffuse Optical Tomography (NIR-DOT) reconstruction, designed to achieve robust, layout-agnostic medical imaging reconstruction.

<br/>

### âœ¨ Key Features

<div align="center">

| ğŸ¯ **Two-Stage Learning** | ğŸ§  **Transformer Innovation** | ğŸ“ **Geometry Awareness** |
|:-------------------------:|:-----------------------------:|:-------------------------:|
| Decoupled spatial and geometric learning | Applies transformer attention to DOT inverse problems | Source-detector coordinate integration |
| **ğŸ”„ Layout Agnostic** | **ğŸ›ï¸ Configurable Pipeline** | **ğŸš€ High Performance** |
| Generalizes across different probe configurations | Easy stage switching and hyperparameter tuning | Optimized for GPU training with mixed precision |

</div>

<br/>

---

## ğŸ—ï¸ Architecture Overview

<div align="center">

**Our hybrid architecture seamlessly integrates CNN spatial learning with transformer geometric awareness**

<br/>

<img src="figs/architecture.png" alt="Two-Stage CNN-Transformer Architecture" width="600"/>

<br/>

*ğŸ¯ **Stage 1**: CNN Autoencoder learns robust spatial priors from 3D volumes*  
*ğŸ§  **Stage 2**: Transformer maps NIR measurements to latent space with geometry awareness*

</div>

<br/>

---

## ğŸ“ Repository Structure

ğŸ—‚ï¸ **Explore the codebase structure**

```
mah422/                             # ğŸ  Root directory
â”œâ”€â”€ ğŸ’¾ checkpoints/                 # Model checkpoints
â”‚   â”œâ”€â”€ checkpoint_stage1_*.pt
â”‚   â””â”€â”€ checkpoint_stage2_*.pt
â”œâ”€â”€ ğŸ§¬ code/                        # Core implementation (~14k LOC)
â”‚   â”œâ”€â”€ ğŸ“Š data_processing/         # Data generation & loading
â”‚   â”‚   â”œâ”€â”€ data_loader.py         # PyTorch DataLoaders
â”‚   â”‚   â””â”€â”€ data_simulator.py      # Phantom simulation
â”‚   â”œâ”€â”€ ğŸ—ï¸ models/                  # Neural architectures
â”‚   â”‚   â”œâ”€â”€ cnn_autoencoder.py     # 3D CNN autoencoder
â”‚   â”‚   â”œâ”€â”€ transformer_encoder.py # Transformer components
â”‚   â”‚   â”œâ”€â”€ hybrid_model.py        # Combined architecture
â”‚   â”‚   â”œâ”€â”€ global_pooling_encoder.py # Global feature extraction
â”‚   â”‚   â””â”€â”€ spatially_aware_embedding.py  # Geometry processing
â”‚   â”œâ”€â”€ ğŸ¯ training/                # Training pipelines
â”‚   â”‚   â”œâ”€â”€ train_hybrid_model.py  # Main training entry point
â”‚   â”‚   â”œâ”€â”€ stage1_trainer.py      # CNN pretraining
â”‚   â”‚   â”œâ”€â”€ stage2_trainer.py      # Transformer training
â”‚   â”‚   â”œâ”€â”€ training_config.py     # Hyperparameters
â”‚   â”‚   â”œâ”€â”€ training_utils.py      # Training utilities
â”‚   â”‚   â”œâ”€â”€ latent_stats.py        # Latent space analysis
â”‚   â”‚   â””â”€â”€ teacher_stage1.py      # Teacher model wrapper
â”‚   â””â”€â”€ ğŸ”§ utils/                   # Utilities & metrics
â”‚       â”œâ”€â”€ metrics.py             # Evaluation metrics
â”‚       â”œâ”€â”€ standardizers.py       # Data normalization
â”‚       â”œâ”€â”€ viz_recon.py          # Visualization
â”‚       â””â”€â”€ logging_config.py      # Centralized logging
â”œâ”€â”€ ğŸ’¾ data/                        # Generated datasets (HDF5)
â”œâ”€â”€ ğŸ“Š figs/                        # Static figures
â”œâ”€â”€ ğŸ“ logs/                        # Training logs
â”œâ”€â”€ ğŸ”§ nirfaster-FF/               # Forward modeling library
â”œâ”€â”€ âš™ï¸ setup/                       # Environment setup
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ bootstrap_lambdalabs.sh   # Remote setup script
â””â”€â”€ ğŸ“Š wandb/                      # W&B experiment tracking
```

---

## ğŸš€ Quickstart

<div align="center">

**âš¡ Get up and running in under 5 minutes**

</div>

<br/>

### 1ï¸âƒ£ Environment Setup

<table>
<tr>
<td width="50%">

**ğŸ’» Local Development**

```bash
# Clone the repository
git clone https://github.com/maxhartml/mah422.git
cd mah422

# Create virtual environment
python3 -m venv env_diss
source env_diss/bin/activate

# Install dependencies
pip install -r setup/requirements.txt
```

</td>
<td width="50%">

**â˜ï¸ Remote Training**

```bash
# SSH into remote instance
ssh -p <PORT> <USER>@<IP>

# Quick setup
git clone https://github.com/maxhartml/mah422.git && cd mah422
source setup/bootstrap_lambdalabs.sh

# Start persistent session
tmux new -s nir-dot-training
```

</td>
</tr>
</table>

<br/>

### 2ï¸âƒ£ Data Generation

```bash
# Generate synthetic phantom dataset
python -m code.data_processing.data_simulator

# Expected output structure:
# data/phantom_001/phantom_001_scan.h5  # Measurements + ground truth
# data/phantom_002/phantom_002_scan.h5
# ...
```

<br/>

### 3ï¸âƒ£ Training Pipeline

<div align="center">

<table>
<tr>
<td width="50%" align="center">

**ğŸ¯ Stage 1: CNN Autoencoder**

```bash
# Configure Stage 1 in training_config.py
# CURRENT_TRAINING_STAGE = "stage1"

python -m code.training.train_hybrid_model
```

*Learns robust spatial priors from 3D volumes*

</td>
<td width="50%" align="center">

**ğŸ§  Stage 2: Transformer**

```bash  
# Configure Stage 2 in training_config.py
# CURRENT_TRAINING_STAGE = "stage2" 

python -m code.training.train_hybrid_model
```

*Maps NIR measurements to learned latent space*

</td>
</tr>
</table>

</div>

<br/>

### 4ï¸âƒ£ Results & Monitoring

<div align="center">

| ğŸ“Š **Checkpoints** | ğŸ“ **Logs** | ğŸ” **Tracking** |
|:------------------:|:-----------:|:---------------:|
| `checkpoints/checkpoint_stage*_*.pt` | `logs/training/` | W&B: `nir-dot-reconstruction` |
| Model states saved automatically | Training & data processing logs | Optional experiment tracking |

</div>

---

## ğŸ”§ Configuration & Training

<div align="center">

**âš™ï¸ Fine-tune your training with comprehensive configuration options**

</div>

<br/>

### ğŸ›ï¸ Key Configuration Parameters

<div align="center">

<table>
<tr>
<td width="33%" align="center">

**ğŸ—ï¸ Architecture**

```python
LATENT_DIM = 256
EMBED_DIM = 256
N_MEASUREMENTS = 256
```

*Core model dimensions*

</td>
<td width="33%" align="center">

**ï¿½ Training**

```python
STAGE1_EPOCHS = 150
STAGE2_EPOCHS = 100
BATCH_SIZE = 4
```

*Training hyperparameters*

</td>
<td width="33%" align="center">

**âš¡ Optimization**

```python
STAGE1_BASE_LR = 1e-4
STAGE2_BASE_LR = 5e-5
USE_EMA = True
```

*Learning & optimization*

</td>
</tr>
</table>

</div>

ğŸ“‹ **Complete Configuration Reference** (training_config.py)

```python
# Stage Control
CURRENT_TRAINING_STAGE = "stage1"  # or "stage2"

# Architecture 
LATENT_DIM = 256                    # Latent space dimension
EMBED_DIM = 256                     # Transformer embedding dim
N_MEASUREMENTS = 256                # Subsampled measurements per phantom

# Training
STAGE1_EPOCHS = 150                 # CNN pretraining epochs  
STAGE2_EPOCHS = 100                 # Transformer training epochs
BATCH_SIZE = 4                      # Batch size (memory dependent)

# Optimization
STAGE1_BASE_LR = 1e-4              # CNN base learning rate
STAGE2_BASE_LR = 5e-5              # Transformer base learning rate
USE_EMA = True                     # Exponential moving average
```

<br/>

---

## ğŸ”¬ Research & Innovation

<div align="center">

**ğŸ“ Academic contributions to medical imaging and deep learning**

</div>

<br/>

### ğŸ“š Scientific Contributions

<div align="center">

<table>
<tr>
<td width="50%" align="center">

**ğŸ—ï¸ Novel Two-Stage Architecture**  
Decoupled spatial and geometric learning for improved generalization

**ğŸ§  Transformer for DOT**  
Applies transformer attention mechanisms to DOT inverse problems

</td>
<td width="50%" align="center">

**ğŸ“ Geometry Integration**  
Source-detector coordinate awareness for layout-agnostic reconstruction

**ğŸ“Š Comprehensive Evaluation**  
Physics-based metrics in raw units with proper statistical reporting

</td>
</tr>
</table>

</div>

<br/>

### ğŸ“ Academic Context

ğŸ“– **Citation & Publication**

```bibtex
@mastersthesis{hart2025nir_dot_hybrid,
  title={Towards Generalisable Inverse Modelling for Frequency-Domain Diffuse Optical Tomography via a Hybrid CNNâ€“Transformer},
  author={Max Andrew Hart},
  school={University of Birmingham},  
  year={2025},
  type={MSc Dissertation},
  note={AI and Machine Learning}
}
```

<br/>

### ğŸ”— References & Acknowledgements

<div align="center">

<table>
<tr>
<td width="33%" align="center">

**ğŸ› ï¸ Technical**
- NIRFASTer-FF Framework
- University of Birmingham
- Vast.ai GPU Platform

</td>
<td width="33%" align="center">

**ğŸ™ Supervision**
- **Dr. Hamid Dehghani**
- Research guidance & supervision

</td>
<td width="33%" align="center">

**ğŸ“š Foundation**
- **Dr. Robin Dale**
- Foundational work & research base

</td>
</tr>
</table>

</div>

<br/>

---

## ğŸ› ï¸ Development & Contributing

### ğŸ¤ Contributing

We welcome contributions! Please see our contribution guidelines:

1. ğŸ´ Fork the repository
2. ğŸŒŸ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’« Commit changes (`git commit -m 'Add amazing feature'`)
4. ğŸš€ Push to branch (`git push origin feature/amazing-feature`)
5. ğŸ”„ Open a Pull Request

---

## ğŸ“‹ Requirements & Compatibility

<div align="center">

**ğŸ’» System specifications and dependency overview**

</div>

<br/>

### ï¿½ï¸ System Requirements

<div align="center">

<table>
<tr>
<td width="25%" align="center">

**ğŸ Python**  
3.10+

</td>
<td width="25%" align="center">

**ğŸš€ GPU**  
CUDA-capable  
(16GB+ VRAM)

</td>
<td width="25%" align="center">

**ğŸ’¾ RAM**  
32GB+  
for full datasets

</td>
<td width="25%" align="center">

**ğŸ’¿ Storage**  
10GB+  
datasets & checkpoints

</td>
</tr>
</table>

</div>

<br/>

### ğŸ“¦ Key Dependencies

ğŸ” **Complete Package List**

<div align="center">

<table>
<tr>
<td width="33%">

**ğŸ§  Core ML/DL**
- `torch: 2.5.1`
- `numpy: 1.26.4`

</td>
<td width="33%">

**ğŸ“Š Data & Visualization**  
- `h5py: 3.10.0`
- `matplotlib: 3.8.2`
- `scipy: 1.11.4`

</td>
<td width="33%">

**ğŸ› ï¸ ML Tools**
- `scikit-learn: 1.3.2`
- `wandb: 0.16.1` (optional)
- `psutil: 5.9.6`

</td>
</tr>
</table>

</div>

<br/>

---

## ğŸ“ Support & Community

<div align="center">

**ğŸ’¬ Connect with us for support, questions, and contributions**

</div>

<br/>

### ğŸ¤ Get Help & Contribute

<div align="center">

<table>
<tr>
<td width="33%" align="center">

**ğŸ“§ Contact**  
[maxhartml@outlook.com](mailto:maxhartml@outlook.com)  

*Direct email support*

</td>
<td width="33%" align="center">

**ğŸ› Issues**  
[GitHub Issues](https://github.com/maxhartml/mah422/issues)  

*Bug reports & feature requests*

</td>
<td width="33%" align="center">

**ğŸ’¡ Discussions**  
[GitHub Discussions](https://github.com/maxhartml/mah422/discussions)  

*Community Q&A*

</td>
</tr>
</table>

</div>

<br/>

### ğŸ·ï¸ Project Information

<div align="center">

| **Latest Release** | **Total Commits** | **Main Branch** | **License** |
|:------------------:|:-----------------:|:---------------:|:-----------:|
| v1.0.0 | 207+ | `main` | MIT |

</div>

<br/>

---

<div align="center">

### ğŸ‰ Thank you for your interest in our NIR-DOT research!

**If this work helps your research, please â­ star this repository!**

<br/>

*Made with â¤ï¸ at the University of Birmingham*

</div>