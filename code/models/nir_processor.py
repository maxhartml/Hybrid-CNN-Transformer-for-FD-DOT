#!/usr/bin/env python3
"""
NIR Measurement Processor with Spatial Attention.

This module implements optimized NIR measurement processing using spatial attention
instead of simple mean pooling. The processor         self.enhanced_projection = nn.Sequential(
            nn.Linear(NIR_INPUT_DIM + TISSUE_CONTEXT_DIM, 48),    # 8 NIR + 8 tissue = 16D input
            nn.ReLU(),
            nn.Linear(48, SPATIAL_EMBED_DIM)
        )cts the spatial relationships
between source-detector pairs and can work with or without tissue context.

Classes:
    SpatialAttentionNIRProcessor: Main NIR processor with spatial attention
    PerMeasurementTissueEncoder: Encoder for individual measurement tissue patches

Author: Max Hart
Date: August 2025
"""

# =============================================================================
# IMPORTS
# =============================================================================

# Standard library imports
import logging
from typing import Optional, Dict, Tuple

# Third-party imports
import torch
import torch.nn as nn
import torch.nn.functional as F

# Project imports
from code.utils.logging_config import get_model_logger

# =============================================================================
# CONSTANTS
# =============================================================================

# NIR measurement configuration
NIR_INPUT_DIM = 8                       # [log_amp, phase, src_x, src_y, src_z, det_x, det_y, det_z]
POSITION_DIM = 6                        # Source and detector positions (xyz each)
MEASUREMENT_DIM = 2                     # Log amplitude and phase
N_MEASUREMENTS = 256                    # Number of measurements per phantom

# Output dimensions
CNN_FEATURE_DIM = 256                   # Must match CNN autoencoder feature dimension
TISSUE_CONTEXT_DIM = 8                  # Output from tissue encoder (2 patches √ó 4D each)

# Spatial attention configuration  
SPATIAL_EMBED_DIM = 256                 # Embedding dimension for spatial attention
NUM_ATTENTION_HEADS = 8                 # Number of attention heads
POSITION_ENCODING_DIM = 64              # Dimension for positional encoding

# Initialize module logger
logger = get_model_logger(__name__)

# =============================================================================
# NIR MEASUREMENT PROCESSORS
# =============================================================================

class PerMeasurementTissueEncoder(nn.Module):
    """
    Encoder for tissue patches associated with individual measurements.
    
    Processes tissue patches around source and detector locations for each
    measurement, providing local anatomical context that gets appended to
    the NIR measurement features.
    """
    
    def __init__(self, patch_size: int = 16, output_dim: int = 4):
        super().__init__()
        
        logger.info(f"üèóÔ∏è  Initializing PerMeasurementTissueEncoder: patch_size={patch_size}, output_dim={output_dim}")
        
        self.patch_size = patch_size
        self.output_dim = output_dim
        
        # Advanced CNN for 16¬≥ patches (2 tissue property channels)
        # Deep residual architecture with batch normalization and dropout
        self.patch_encoder = nn.Sequential(
            # Stage 1: 16¬≥ ‚Üí 8¬≥ (initial feature extraction)
            nn.Conv3d(2, 16, kernel_size=3, padding=1, bias=False),   # 2 ‚Üí 16 channels
            nn.BatchNorm3d(16),
            nn.ReLU(inplace=True),
            nn.Conv3d(16, 16, kernel_size=3, padding=1, bias=False),  # Residual-style depth
            nn.BatchNorm3d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),                   # 16¬≥ ‚Üí 8¬≥
            nn.Dropout3d(0.1),
            
            # Stage 2: 8¬≥ ‚Üí 4¬≥ (intermediate features)
            nn.Conv3d(16, 32, kernel_size=3, padding=1, bias=False), # 16 ‚Üí 32 channels
            nn.BatchNorm3d(32),
            nn.ReLU(inplace=True),
            nn.Conv3d(32, 32, kernel_size=3, padding=1, bias=False), # Residual-style depth
            nn.BatchNorm3d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),                   # 8¬≥ ‚Üí 4¬≥
            nn.Dropout3d(0.1),
            
            # Stage 3: 4¬≥ ‚Üí 2¬≥ (high-level features)
            nn.Conv3d(32, 64, kernel_size=3, padding=1, bias=False), # 32 ‚Üí 64 channels
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.Conv3d(64, 64, kernel_size=3, padding=1, bias=False), # Residual-style depth
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=2, stride=2),                   # 4¬≥ ‚Üí 2¬≥
            nn.Dropout3d(0.15),
            
            # Global feature aggregation
            nn.AdaptiveAvgPool3d(1),                                 # 2¬≥ ‚Üí 1¬≥ (global pooling)
            nn.Flatten(),                                            # ‚Üí 64D feature vector
            
            # Feature projection with residual-style MLP
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(16, output_dim)                                # ‚Üí output_dim per patch
        )
        
        # Initialize weights
        self._init_weights()
        
        logger.info(f"‚úÖ PerMeasurementTissueEncoder initialized with ~{self.count_parameters()} parameters")
    
    def count_parameters(self):
        """Count total parameters in the model."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def _init_weights(self):
        """Initialize model weights."""
        for m in self.modules():
            if isinstance(m, (nn.Conv3d, nn.Linear)):
                nn.init.normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, tissue_patches: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through per-measurement tissue encoder.
        
        Args:
            tissue_patches (torch.Tensor): Shape [batch, 2, 16^3*2]
                batch_size measurements √ó 2 patches (source + detector) √ó concatenated patch data
                where concatenated patch data is [Œº_a_flat + Œº_s_flat] for 16¬≥ voxels
        
        Returns:
            torch.Tensor: Shape [batch, 8] (4D per source + 4D per detector)
        """
        logger.debug(f"üèÉ PerMeasurementTissueEncoder forward: input shape {tissue_patches.shape}")
        
        # Handle both batched and single phantom cases
        original_shape = tissue_patches.shape
        if len(original_shape) == 4:
            # Batched: [batch_size, n_measurements, n_patches_per_measurement, concatenated_size]
            batch_size, n_measurements, n_patches, concatenated_size = original_shape
            # Reshape to [batch_size * n_measurements, n_patches, concatenated_size] for processing
            tissue_patches = tissue_patches.view(-1, n_patches, concatenated_size)
            process_batch_size = batch_size * n_measurements
            is_batched = True
        else:
            # Single phantom: [n_measurements, n_patches, concatenated_size]
            process_batch_size, n_patches, concatenated_size = original_shape
            batch_size, n_measurements = None, None
            is_batched = False
        
        # FIXED: Handle concatenated format [Œº_a_flat + Œº_s_flat] not interleaved
        patch_volume = self.patch_size ** 3  # 16^3 = 4096
        n_channels = 2  # absorption + scattering
        
        # Split concatenated data: [Œº_a_flat + Œº_s_flat] ‚Üí separate channels
        # Expected: concatenated_size = patch_volume * 2 = 8192
        assert concatenated_size == patch_volume * 2, f"Expected {patch_volume * 2} elements, got {concatenated_size}"
        
        # Split into absorption and scattering channels
        absorption_flat = tissue_patches[:, :, :patch_volume]  # [batch, 2, 4096]
        scattering_flat = tissue_patches[:, :, patch_volume:]  # [batch, 2, 4096]
        
        # Reshape each channel to 3D: [process_batch, 2, 4096] ‚Üí [process_batch, 2, 16, 16, 16]
        absorption_3d = absorption_flat.view(process_batch_size, n_patches, self.patch_size, self.patch_size, self.patch_size)
        scattering_3d = scattering_flat.view(process_batch_size, n_patches, self.patch_size, self.patch_size, self.patch_size)
        
        # Stack channels: [process_batch, 2, 2, 16, 16, 16]
        reshaped_patches = torch.stack([absorption_3d, scattering_3d], dim=2)
        
        logger.debug(f"üì¶ Reshaped patches: {reshaped_patches.shape}")
        
        # Reshape for batch processing: [batch√ó2, 2, 16, 16, 16]
        patches = reshaped_patches.view(-1, n_channels, self.patch_size, self.patch_size, self.patch_size)
        logger.debug(f"üì¶ Patches for CNN: {patches.shape}")
        
        # Log patch content and validate
        if logger.isEnabledFor(logging.DEBUG):
            zero_ratio = (patches == 0).float().mean()
            nonzero_count = (patches != 0).sum()
            zero_percentage = zero_ratio.item() * 100
            
            # VALIDATION WARNINGS: Alert if batch has unusual zero content
            if zero_percentage > 75.0:
                logger.warning(f"‚ö†Ô∏è HIGH ZEROS: Tissue patch batch has {zero_percentage:.1f}% zeros (>75%)")
            elif zero_percentage < 25.0:
                logger.warning(f"‚ö†Ô∏è LOW ZEROS: Tissue patch batch has {zero_percentage:.1f}% zeros (<25%)")
            
            logger.debug(f"üìä Patch content: {zero_percentage:.1f}% zeros, {nonzero_count} non-zero values")
            if nonzero_count > 0:
                nonzero_patches = patches[patches != 0]
                logger.debug(f"üìä Non-zero range: [{nonzero_patches.min():.4f}, {nonzero_patches.max():.4f}]")
        
        # Encode all patches: [batch√ó2, output_dim]
        encoded_patches = self.patch_encoder(patches)
        logger.debug(f"üì¶ Encoded patches: {encoded_patches.shape}")
        
        # Reshape back: [process_batch, 2, output_dim] ‚Üí [process_batch, 2*output_dim]
        encoded_patches = encoded_patches.view(process_batch_size, n_patches, self.output_dim)
        tissue_contexts = encoded_patches.view(process_batch_size, n_patches * self.output_dim)
        
        # If input was batched, reshape back to [batch_size, n_measurements, features]
        if is_batched:
            tissue_contexts = tissue_contexts.view(batch_size, n_measurements, n_patches * self.output_dim)
        
        logger.debug(f"üì¶ PerMeasurementTissueEncoder output: {tissue_contexts.shape}")
        return tissue_contexts


class SimplifiedNIRProcessor(nn.Module):
    """
    Simplified NIR measurement processor without redundant attention.
    
    Processes NIR measurements using simple projections with spatial encoding.
    The transformer encoder handles all attention mechanisms, making this
    processor focused solely on feature projection and spatial awareness.
    Supports both baseline mode (NIR only) and enhanced mode (NIR + tissue context).
    
    Architectural Benefits:
    - Removes redundant attention (~330K parameters saved)
    - Clearer separation of concerns (projection vs attention)
    - All spatial reasoning handled by transformer
    - Maintains dual-path architecture for baseline/enhanced modes
    """
    
    def __init__(self):
        super().__init__()
        
        logger.info("üèóÔ∏è  Initializing SimplifiedNIRProcessor")
        
        # Tissue encoder for per-measurement context
        self.tissue_encoder = PerMeasurementTissueEncoder()
        
        # Different projections for baseline vs enhanced mode
        self.baseline_projection = nn.Sequential(
            nn.Linear(NIR_INPUT_DIM, 128),  # 8 ‚Üí 128
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, CNN_FEATURE_DIM)  # 128 ‚Üí 256
        )
        
        self.enhanced_projection = nn.Sequential(
            nn.Linear(NIR_INPUT_DIM + TISSUE_CONTEXT_DIM, 96),  # 16 ‚Üí 96  
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(96, CNN_FEATURE_DIM)  # 96 ‚Üí 256
        )
        
        # Spatial encoding for source/detector locations (preserved from original)
        self.spatial_encoder = nn.Sequential(
            nn.Linear(POSITION_DIM, 32),  # 6 ‚Üí 32
            nn.ReLU(),
            nn.Linear(32, 64)  # 32 ‚Üí 64
        )
        
        # Initialize weights
        self._init_weights()
        
        logger.info(f"‚úÖ SimplifiedNIRProcessor initialized with ~{self.count_parameters()} parameters")
    
    def count_parameters(self):
        """Count total parameters in the model."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def _init_weights(self):
        """Initialize model weights."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def create_spatial_encoding(self, nir_measurements: torch.Tensor) -> torch.Tensor:
        """
        Create spatial encoding from source/detector coordinates.
        
        Args:
            nir_measurements (torch.Tensor): Shape [batch, 8]
                Last 6 dimensions are [src_x, src_y, src_z, det_x, det_y, det_z]
        
        Returns:
            torch.Tensor: Spatial encodings [batch, 64]
        """
        # Extract positions from NIR measurements: [batch, 8] ‚Üí [batch, 6]
        positions = nir_measurements[:, 2:]  # [batch, 6]
        
        # Encode spatial positions
        spatial_encoding = self.spatial_encoder(positions)  # [batch, 64]
        
        return spatial_encoding
    
    def forward(self, nir_measurements: torch.Tensor, 
                tissue_patches: Optional[torch.Tensor] = None, 
                use_tissue_patches: bool = False) -> Dict[str, torch.Tensor]:
        """
        Simplified forward pass through NIR processor.
        
        Args:
            nir_measurements (torch.Tensor): Shape [batch, 8] (individual measurements)
            tissue_patches (torch.Tensor, optional): Shape [batch, 2, 16^3*2] (individual tissue patches)
            use_tissue_patches (bool): Whether to use tissue context
        
        Returns:
            Dict[str, torch.Tensor]: Dictionary containing:
                - 'features': Processed features [batch, CNN_FEATURE_DIM]
                - 'spatial_encoding': Spatial encoding [batch, 64] 
                - 'projected_measurements': Projected measurements [batch, CNN_FEATURE_DIM]
        """
        logger.debug(f"üèÉ SimplifiedNIRProcessor forward: nir_measurements {nir_measurements.shape}")
        
        batch_size = nir_measurements.shape[0]
        
        if use_tissue_patches and tissue_patches is not None:
            logger.debug("üì¶ Enhanced mode: processing tissue context")
            # Enhanced mode: append tissue context to each measurement
            tissue_contexts = self.tissue_encoder(tissue_patches)  # [batch, 8]
            enhanced_measurements = torch.cat([nir_measurements, tissue_contexts], dim=1)  # [batch, 16]
            projected = self.enhanced_projection(enhanced_measurements)  # [batch, 256]
        else:
            logger.debug("üì¶ Baseline mode: NIR measurements only")
            # Baseline mode: NIR measurements only
            projected = self.baseline_projection(nir_measurements)  # [batch, 256]
        
        logger.debug(f"üì¶ Projected measurements: {projected.shape}")
        
        # Create spatial encoding from source/detector positions
        spatial_encoding = self.create_spatial_encoding(nir_measurements)  # [batch, 64]
        logger.debug(f"üì¶ Spatial encoding: {spatial_encoding.shape}")
        
        # No attention mechanism - let transformer handle all spatial reasoning
        # Return projected features directly
        
        return {
            'features': projected,  # [batch, CNN_FEATURE_DIM]
            'spatial_encoding': spatial_encoding,  # [batch, 64] - available for transformer if needed
            'projected_measurements': projected  # [batch, CNN_FEATURE_DIM]
        }
