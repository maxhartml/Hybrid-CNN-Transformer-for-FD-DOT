\chapter{Literature Review}

\section{Conventional Foundations in FD-DOT}
This section establishes the minimum physics and algorithmic background required for the learning-based approach pursued later. It traces the standard modelling pipeline from radiative transport to the diffusion approximation, outlines frequency-domain forward modelling with the finite element method (FEM), and summarises classical inverse formulations and regularisation strategies that motivate data-driven alternatives.

\subsection{From radiative transport to diffusion}
At its most fundamental level, photon propagation in tissue is governed by the radiative transport equation (RTE), which evolves radiance over space and angle. The RTE is accurate but high-dimensional and computationally demanding, rendering direct inversion impractical for typical DOT settings \cite{arridge1999}. In highly scattering media—typical of the NIR window ($650$–$900$\,nm)—the angular distribution rapidly approaches isotropy; under this regime, the RTE admits the diffusion approximation, which describes photon fluence via a parabolic partial differential equation \cite{arridge1999, gibson2005, boas2001}. The diffusion model provides a tractable surrogate that is sufficiently accurate away from boundaries or low-scattering regions; its known limitations (e.g., near refractive index mismatches and superficial layers) are well documented \cite{arridge2009}. For the frequency-domain case considered here, sinusoidal modulation leads to a complex-valued diffusion equation whose solutions yield amplitude attenuation and phase shift at the tissue boundary—precisely the observables used in FD-DOT.

\subsection{Frequency-domain forward modelling (FEM in practice)}
Realistic head, breast, and organ geometries require numerical solvers. FEM has become the de facto approach in DOT due to its ability to accommodate complex boundaries and heterogeneous optical properties \cite{dehghani2009}. The tissue domain is discretised into elements; material parameters are piecewise defined, and the weak form of the (complex) diffusion equation is assembled into large sparse linear systems. The resulting forward operator $\mathcal{F}$ maps spatial fields of absorption $\mu_a$ (mm$^{-1}$) and reduced scattering $\mu'_s$ (mm$^{-1}$) to boundary measurements of $\log A$ (log-amplitude) and $\phi$ (phase). Alternative schemes—finite differences, boundary elements, Monte Carlo accelerations—exist, but FEM remains dominant given its mesh flexibility, ease of incorporating boundary conditions, and direct compatibility with anatomical priors (e.g., from MRI/CT) and frequency-domain extensions \cite{dehghani2009}. The computational burden is substantial: a single forward solve entails factorising or iteratively solving very large systems, and sensitivity (Jacobian) computations compound costs when performed repeatedly in inversion. These factors underpin the latency observed in conventional FD-DOT pipelines and motivate learned surrogates or end-to-end predictors.

\subsection{Classical inverse formulations and regularisation}
DOT reconstruction seeks spatial maps of $(\mu_a,\mu'_s)$ from sparse, surface-only data. A standard variational formulation is
\[
\hat{\mu} \;=\; \arg\min_{\mu}\;\big\| \mathbf{y} - \mathcal{F}(\mu) \big\|_2^2 \;+\; \lambda\,R(\mu),
\]
where $\mathbf{y}$ denotes measured $\log A$ and $\phi$, $R(\mu)$ encodes prior structure, and $\lambda>0$ balances data fidelity and regularity. The prevalent solvers linearise the forward map about the current estimate and iterate with Gauss–Newton or Levenberg–Marquardt updates, using FEM-derived Jacobians to compute sensitivities \cite{arridge1999}. While effective, these methods incur minutes-per-volume runtimes in 3D and can be brittle in low signal-to-noise regimes.

Regularisation is therefore central. Tikhonov (quadratic) priors suppress noise but blur edges; sparsity and total variation (TV) constraints better preserve localised inclusions at the expense of non-smooth optimisation and parameter tuning \cite{gibson2005}. Bayesian formulations (e.g., MAP estimation and approximation‐error modelling) offer principled uncertainty handling and hierarchical priors but further increase computational cost \cite{tarvainen2010boe}. When available, anatomical priors from MRI/CT constrain the solution space and sharpen localisation, but they tie DOT to external imaging and do not remove the need for repeated Jacobian evaluations \cite{arridge1999}. Across these variants, three limitations recur: (i) heavy reliance on hand-crafted priors and manual hyperparameter tuning, (ii) sensitivity to model–data mismatch (e.g., boundary conditions, optical heterogeneity), and (iii) prohibitive latency stemming from large-scale PDE solves and sensitivity computations.

\paragraph{Takeaway.} The diffusion approximation and FEM provide a physically grounded, widely validated forward model for FD-DOT, and classical inverse solvers remain a valuable reference. However, their computational cost and dependence on hand-crafted priors motivate learning-based approaches that can embed data-driven regularity and achieve near real-time inference once trained. The next section therefore reviews learning-based reconstruction for FD-DOT and related modalities, with emphasis on architectures and representations that address geometry variability, noise, and domain shift.

\section{Learning-Based Reconstruction for FD-DOT}
Deep learning (DL) offers two capabilities that directly address the ill-posedness and latency of FD-DOT: the capacity to learn strong, data-driven priors from examples, and the ability to amortise computation so that inference is near real time once training is complete. Inverse problems in other modalities have shown that learned priors can stabilise reconstructions under noise and sparsity, either by unrolling iterative solvers into trainable networks or by learning direct sensor-to-image mappings; this section focuses on how these ideas translate to FD-DOT.

\subsection{Why learning helps FD-DOT}
Across CT, MRI, and photoacoustics, DL has improved reconstructions via two patterns: \emph{(i) unrolling} classical optimisation into networks whose layers mirror iterations (imparting interpretability and data‐consistency), and \emph{(ii) direct inversion} from raw or minimally processed measurements to images. Reviews by Monga \emph{et~al.} and Arridge \emph{et~al.} synthesise the mathematical foundations of learned inverse problems, including the role of stability, data consistency, and the benefits/risks of data-driven priors \cite{monga2021, arridge2019}. Unrolled methods such as the Learned Primal–Dual algorithm of Adler and Öktem exemplify how physics and learning can be combined to handle non-linear forward operators while remaining task-specific and efficient at test time \cite{adler2018}. These precedents justify a learning-based approach for FD-DOT, where the forward model is expensive and the measurements are sparse and noisy.

\subsection{DOT-specific DL: what has worked, and what has not}
Early DOT studies adopted convolutional encoder–decoders (e.g., U-Nets) to map boundary measurements to absorption (and sometimes scattering) images on simulated or phantom datasets, reporting sharper inclusions and large speed-ups over Tikhonov-regularised FEM \cite{feng2020}. More recent work has scaled to 3D and demonstrated feasibility in vivo: Deng \emph{et~al.} presented a three-module network (“FDU-net”) trained on simulations and validated on human breast data, showing improved anomaly localisation while maintaining sub-second inference \cite{deng2023}. End-to-end frameworks for diffuse optical imaging have also appeared (e.g., Periodic-net), indicating the generality of learned reconstructions for optical boundary data \cite{murad2023}. Despite these advances, limitations persist: most published models assume fixed probe geometries, modest datasets, and restricted anatomical variability, leading to degradation under geometry shift and to fragility when moving from simulation to experiment. These gaps motivate architectures that encode geometry explicitly and training regimes that expose models to broader phantom and probe distributions.

\subsection{CNN encoders/decoders for volumetric DOT}
CNNs remain attractive for DOT because they learn spatially local, translation-equivariant priors over volumetric images. In 3D FD-DOT, encoder–decoder topologies (including U-Nets and autoencoders) provide strong inductive bias for recovering compact inclusions and smooth backgrounds. Practical trade-offs arise between resolution and latency (3D convolutions are costly) and between single-parameter networks and multi-head designs that jointly estimate $(\mu_a,\mu_s')$. A useful pattern—adopted in this dissertation—is to learn a \emph{spatial latent} with a 3D CNN (via autoencoding ground-truth volumes), then condition or regress that latent from measurements. This separates the tasks of (i) capturing anatomical/structural priors and (ii) fusing measurement information, and it aligns with evidence that learned spatial priors stabilise DOT reconstructions while reducing sensitivity to noise in boundary data.

\subsection{Attention and transformers for measurement sequences}
FD-DOT measurements naturally form sets or sequences of source–detector (SD) pairs, each described by $(\log A,\phi)$ and explicit coordinates of source and detector positions. Self-attention is well suited to aggregate such tokens: it models long-range dependencies across SD pairs, is permutation-flexible, and can integrate explicit spatial embeddings to encode geometry. Positional or geometry-aware embeddings (e.g., from SD coordinates and separations) allow the network to remain path-agnostic while still being \emph{geometry-aware}, addressing the common failure mode under layout changes. Attention pooling further provides global context, which is important when deeper structures only influence long-separation measurements weakly. A constraint for transformers is data hunger; this can be mitigated by strong spatial priors (CNN latent spaces) and aggressive geometry randomisation during training.

\subsection{Hybrid two-stage designs for FD-DOT}
Hybrid CNN–Transformer frameworks combine the strengths of spatial priors and sequence aggregation. Dale \emph{et~al.} recently demonstrated high-speed, multi-parameter FD-DOT using a hybrid design that processes measurement sequences with spatial awareness and reconstructs 3D absorption and reduced scattering at sub-second rates \cite{dale2024}. In this two-stage paradigm (also adopted here), Stage~1 learns a volumetric latent representation from ground-truth $(\mu_a,\mu_s')$; Stage~2 consumes SD tokens built from $(\log A,\phi)$ and explicit coordinates, using attention to integrate global information and regress the latent (and hence the volume). Variants include (i) multi-query attention to stabilise global aggregation, (ii) explicit coordinate encodings (SDS, 3D positions, relative vectors), and (iii) selective fine-tuning of the CNN decoder to align the latent space with measurement-conditioned features. The principal benefits are robustness (priors learned from anatomy-like variability) and latency (amortised inference), while typical failure modes—data shift and geometry shift—are addressed by dataset design (phantom/probe diversity, SO(3) rotations) and by geometry-aware tokenisation. This dissertation builds on that direction with systematic geometry randomisation (fixed 256-token sequences), enriched spatial embeddings, and latent alignment strategies aimed at improving generalisation without sacrificing fidelity in both parameters.

\section{Robustness, Baselines, and the Research Gap}
This section consolidates the failure modes that impede deployment of learning-based FD-DOT, reviews a recent path-agnostic baseline, and states the problem this dissertation addresses together with its contributions.

\subsection{Deployment failure modes}

\textit{Geometry shift.} Handheld and wearable probes rarely follow fixed layouts; source–detector (SD) separations vary with operator handling and patient anatomy. Models trained on a single geometry often degrade when SD patterns change because the mapping from $(\log A,\phi)$ to volume depends on the sampling paths through tissue. Classical FEM-based solvers accommodate arbitrary layouts by recomputing Jacobians \cite{arridge1999,dehghani2009}, but many DL-DOT studies assume fixed arrays or limited variability \cite{feng2020,deng2023}. This motivates \emph{explicit} geometry handling in learned models and \emph{systematic} geometry randomisation during training.

\textit{Noise robustness.} FD-DOT measurements are sensitive to coupling, electronic noise, and instrumental drift; small phase errors or amplitude fluctuations can destabilise reconstructions if the learned mapping implicitly overfits to noise patterns. Classical formulations manage this with regularisation and uncertainty modelling \cite{arridge1999,tarvainen2010boe}, whereas learned methods must incorporate noise-aware training and invariances to remain reliable across sessions and systems.

\textit{Simulation-to-real gap.} Synthetic phantoms enable scale but under-represent anatomical heterogeneity, motion artefacts, and hardware imperfections. As a result, models trained on narrow synthetic distributions can fail to transfer to experimental or in vivo data \cite{gibson2005,arridge2009}. Narrowing this gap requires richer phantom distributions, physics-respecting augmentation, and architectures that remain stable under moderate forward-model mismatch.

\subsection{Baseline: path-agnostic DL-DOT}
Dale \emph{et~al.} proposed a hybrid CNN–Transformer framework that processes SD measurements as tokens augmented with explicit spatial information (e.g., source and detector coordinates), enabling the network to integrate arbitrary scanning pathways while reconstructing 3D absorption and reduced scattering at high speed \cite{dale2024}. The approach combines a volumetric prior (learned by a CNN) with a transformer encoder that aggregates measurements via self-attention, achieving multi-parameter reconstructions with sub-second inference and reporting robustness to path variation on held-out layouts \cite{dale2024, dale2025, dale2025thesis}. This baseline establishes two key principles for practical DL-DOT: (i) \emph{geometry-aware tokenisation} to mitigate layout dependence, and (ii) \emph{amortised inference} to meet latency constraints. Remaining challenges are shaped by the training distribution: generalisation can still be limited by the diversity of phantoms, measurement noise models, and the extent of geometry randomisation seen during training.

\subsection{Problem statement and contributions}
Building on the baseline principles above, this dissertation targets \emph{geometry- and anatomy-generalised} FD-DOT reconstruction under realistic noise. The central problem is to learn a mapping from measurement sequences to volumetric optical properties that maintains fidelity in both $\mu_a$ and $\mu'_s$ across widely varying probe layouts and tissue shapes. The specific contributions are:

\begin{enumerate}[leftmargin=*,label=\arabic*.]
    \item \textbf{Diverse phantom and probe generation.} A high-throughput pipeline produces ellipsoidal tissue volumes embedded in air, inducing realistic tissue–air boundaries for surface-aware SD placement. Phantoms undergo random $\mathrm{SO}(3)$ rotations to remove spatial bias, and tumour inclusions vary in size and shape to broaden anatomical priors.
    
    \item \textbf{Systematic geometry randomisation.} Each phantom yields a large set of SD measurements that are dynamically subsampled into fixed 256-token sequences, enforcing invariance to probe placement and providing strong augmentation against geometry shift.
    
    \item \textbf{Hybrid CNN–Transformer with spatially aware embeddings.} Stage~1 learns a 3D spatial latent from ground-truth volumes; Stage~2 aggregates SD tokens built from $(\log A,\phi)$ and explicit coordinates. Multi-query attention stabilises global aggregation, and selective decoder fine-tuning (latent alignment) aligns the spatial prior with measurement-conditioned features, following the hybrid design ethos but developed independently of specific architectural details \cite{dale2024,dale2025,dale2025thesis}.
\end{enumerate}

\noindent\textbf{Hypothesis:} Combining phantom/probe diversity, systematic geometry randomisation, and a hybrid geometry-aware architecture will improve robustness and generalisation across probe layouts and anatomical variability, while preserving fidelity in absorption and scattering reconstructions. By embedding stronger spatial priors and training under controlled variability, the model is expected to deliver reconstructions that are both accurate and efficient, achieving performance competitive with the best reported DL-DOT systems \cite{dale2024}.

\subsection{From gap to methodology}
Chapter~3 introduces the physics-based data pipeline and geometry randomisation protocol, including phantom construction, optical property assignment, probe placement, and noise modelling. Chapter~4 then presents the hybrid CNN–Transformer architecture, followed in Chapter~5 by the training strategy that operationalises it. Chapter~6 reports the experimental results and analysis, with Chapter~7 offering critical discussion and Chapter~8 concluding with reflections and future directions. Together, these chapters translate the research gap into a concrete methodology and evaluation pathway.

