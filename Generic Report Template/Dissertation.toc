\babel@toc {british}{}\relax 
\contentsline {chapter}{Abstract}{ii}{Doc-Start}%
\contentsline {chapter}{Acknowledgements}{iii}{Doc-Start}%
\contentsline {chapter}{Abbreviations}{iv}{Doc-Start}%
\contentsline {chapter}{List of Figures}{ix}{Doc-Start}%
\contentsline {chapter}{List of Tables}{x}{Doc-Start}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Background and Motivation}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Frequency-Domain Diffuse Optical Tomography}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Problem Formulation and Notation}{2}{section.1.3}%
\contentsline {section}{\numberline {1.4}Challenges in FD-DOT Reconstruction}{4}{section.1.4}%
\contentsline {section}{\numberline {1.5}Research Objectives and Contributions}{4}{section.1.5}%
\contentsline {chapter}{\numberline {2}Literature Review}{6}{chapter.2}%
\contentsline {section}{\numberline {2.1}Conventional Foundations in FD-DOT}{6}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}From radiative transport to diffusion}{6}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Frequency-domain forward modelling (FEM in practice)}{7}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Classical inverse formulations and regularisation}{7}{subsection.2.1.3}%
\contentsline {paragraph}{Takeaway.}{8}{section*.1}%
\contentsline {section}{\numberline {2.2}Learning-Based Reconstruction for FD-DOT}{8}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Why learning helps FD-DOT}{8}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}DOT-specific DL: what has worked, and what has not}{8}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}CNN encoders/decoders for volumetric DOT}{9}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Attention and transformers for measurement sequences}{9}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Hybrid two-stage designs for FD-DOT}{9}{subsection.2.2.5}%
\contentsline {section}{\numberline {2.3}Robustness, Baselines, and the Research Gap}{10}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Deployment failure modes}{10}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Baseline: path-agnostic DL-DOT}{11}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Problem statement and contributions}{11}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}From gap to methodology}{12}{subsection.2.3.4}%
\contentsline {chapter}{\numberline {3}Physics-Based Synthetic Data Pipeline}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Forward Modelling in the Frequency Domain}{14}{section.3.1}%
\contentsline {section}{\numberline {3.2}Geometric Phantom Construction}{14}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Ellipsoidal Tissue and Inclusion Design}{14}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}SO(3) Rotations and Spatial Bias Mitigation}{14}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Optical Property Assignment}{14}{section.3.3}%
\contentsline {section}{\numberline {3.4}Surface Extraction and Probe Placement}{14}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Binary Morphological Surface Extraction}{14}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Surface-Constrained Source–Detector Placement}{14}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}Probe Geometry Randomisation and Tokenisation (Contribution)}{14}{section.3.5}%
\contentsline {section}{\numberline {3.6}Noise Model}{14}{section.3.6}%
\contentsline {section}{\numberline {3.7}Dataset Composition and Preprocessing}{14}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}Standardisation and Leakage Prevention}{14}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}HDF5 Design and DataLoader Throughput}{14}{subsection.3.7.2}%
\contentsline {chapter}{\numberline {4}Proposed Hybrid CNN–Transformer Model}{15}{chapter.4}%
\contentsline {section}{\numberline {4.1}Architectural Overview}{16}{section.4.1}%
\contentsline {section}{\numberline {4.2}Stage 1: CNN Autoencoder (Teacher Prior)}{16}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Encoder: Residual Blocks and Downsampling}{16}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Latent Space Design ($d_z{=}256$)}{16}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Decoder: Progressive Upsampling and Reconstruction}{16}{subsection.4.2.3}%
\contentsline {section}{\numberline {4.3}Stage 2: Spatially-Aware Measurement Embedding (Contribution)}{16}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Signal Branch for $\{\log A,\varphi \}$}{16}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Position Branch for $\{\mathbf {x}_\mathrm {src},\mathbf {x}_\mathrm {det}\}$}{16}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Fusion and Token Formation}{16}{subsection.4.3.3}%
\contentsline {section}{\numberline {4.4}Transformer Encoder}{16}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Attention, Depth, and Positional Handling}{16}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Regularisation and Capacity Control}{16}{subsection.4.4.2}%
\contentsline {section}{\numberline {4.5}Aggregation via Multi-Query Attention Pooling (Contribution)}{16}{section.4.5}%
\contentsline {section}{\numberline {4.6}Latent Alignment and Decoder Reuse}{16}{section.4.6}%
\contentsline {section}{\numberline {4.7}Inference Pathway and Computational Cost}{16}{section.4.7}%
\contentsline {chapter}{\numberline {5}Training Strategies and Optimisation}{17}{chapter.5}%
\contentsline {section}{\numberline {5.1}Stage 1: CNN Pre-Training}{17}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Loss Function and Schedules}{17}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Stability Tricks (Mixed Precision, Gradient Clipping)}{17}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Stage 2: Transformer Training with Latent Alignment}{17}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Latent RMSE-Only Objective (Contribution)}{17}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Decoder Unfreezing Protocol (Contribution)}{17}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Measurement Subsampling and Augmentation}{17}{subsection.5.2.3}%
\contentsline {subsection}{\numberline {5.2.4}Optimisers, LR Schedules, and Weight Decay}{17}{subsection.5.2.4}%
\contentsline {section}{\numberline {5.3}Implementation Details and Reproducibility}{17}{section.5.3}%
\contentsline {chapter}{\numberline {6}Experimental Results and Analysis}{18}{chapter.6}%
\contentsline {section}{\numberline {6.1}Experimental Setup}{19}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Datasets and Splits}{19}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}Metrics (Latent RMSE; Voxel RMSE/SSIM for $\mu _a,\mu '_s$)}{19}{subsection.6.1.2}%
\contentsline {subsection}{\numberline {6.1.3}Hardware and Runtime Reporting}{19}{subsection.6.1.3}%
\contentsline {section}{\numberline {6.2}Stage 1 Results: Autoencoder Reconstruction Quality}{19}{section.6.2}%
\contentsline {section}{\numberline {6.3}Stage 2 Results: Transformer Enhancement}{19}{section.6.3}%
\contentsline {section}{\numberline {6.4}Ablations (Contribution-Focused)}{19}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Mean Pooling vs Multi-Query Attention}{19}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}Fixed Geometry vs Randomised Geometry + $L{=}256$}{19}{subsection.6.4.2}%
\contentsline {subsection}{\numberline {6.4.3}With vs Without Decoder Unfreezing}{19}{subsection.6.4.3}%
\contentsline {subsection}{\numberline {6.4.4}Embedding Variants: With/Without Position Branch}{19}{subsection.6.4.4}%
\contentsline {subsection}{\numberline {6.4.5}Sequence Length Sensitivity: $L\in \{128,256,512\}$}{19}{subsection.6.4.5}%
\contentsline {section}{\numberline {6.5}Generalisation to Held-Out Probe Layouts (Contribution)}{19}{section.6.5}%
\contentsline {section}{\numberline {6.6}Comparative Analysis with Prior Work (Dale Baseline)}{19}{section.6.6}%
\contentsline {section}{\numberline {6.7}Qualitative Visualisation and Error Analysis}{19}{section.6.7}%
\contentsline {chapter}{\numberline {7}Discussion}{20}{chapter.7}%
\contentsline {section}{\numberline {7.1}Key Findings}{20}{section.7.1}%
\contentsline {section}{\numberline {7.2}Robustness to Geometry and Noise Shift}{20}{section.7.2}%
\contentsline {section}{\numberline {7.3}Clinical Implications of a Geometry-Robust DOT Model}{20}{section.7.3}%
\contentsline {section}{\numberline {7.4}Limitations and Threats to Validity}{20}{section.7.4}%
\contentsline {section}{\numberline {7.5}Computational Efficiency and Practical Deployment}{20}{section.7.5}%
\contentsline {section}{\numberline {7.6}Positioning Against Prior Work (Dale) and Field Impact}{20}{section.7.6}%
\contentsline {chapter}{\numberline {8}Conclusion and Future Work}{21}{chapter.8}%
\contentsline {section}{\numberline {8.1}Summary of Contributions}{21}{section.8.1}%
\contentsline {section}{\numberline {8.2}Future Directions}{21}{section.8.2}%
\contentsline {section}{\numberline {8.3}Final Remarks}{21}{section.8.3}%
\contentsline {chapter}{\numberline {A}Implementation and Hyperparameters}{24}{chapter.1}%
\contentsline {chapter}{\numberline {B}Extended Dataset Examples and Probe Layouts}{25}{chapter.2}%
\contentsline {chapter}{\numberline {C}Additional Quantitative Results}{26}{chapter.3}%
\contentsline {chapter}{\numberline {D}Mathematical Derivations}{27}{chapter.4}%
\contentsline {chapter}{\numberline {E}Reproducibility Checklist and Ethics Statement}{28}{chapter.5}%
