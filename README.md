# ğŸ”¬ Hybrid CNN-Transformer for NIR-DOT Reconstruction

<div align="center">

<img src="data/phantom_01/phantom_001_probe_layout.png" alt="NIR-DOT Architecture" width="500"/>

<h3>ğŸ¯ Towards Generalisable Inverse Modelling for Frequency-Domain DOT<br/>via a Hybrid CNNâ€“Transformer</h3>

[![Python](https://img.shields.io/badge/Python-3.10%2B-3776ab?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](./LICENSE)
[![W&B](https://img.shields.io/badge/Weights%20%26%20Biases-Optional-ffbe00?style=for-the-badge&logo=weightsandbiases&logoColor=black)](https://wandb.ai/)

**Robust, layout-agnostic deep learning for diffuse optical tomography reconstruction**  
*Two-stage architecture: autoencoder spatial prior + transformer geometry mapping*

[ğŸš€ Quick Start](#-quickstart) â€¢ 
[ğŸ“– Documentation](#-documentation) â€¢ 
[ğŸ”¬ Research](#-research) â€¢ 
[ğŸ¤ Contributing](#-contributing)

---

</div>

## ğŸŒŸ Overview

> **Revolutionary approach to NIR-DOT reconstruction combining the spatial learning power of CNNs with the geometric awareness of transformers.**

This repository implements a novel **two-stage deep learning pipeline** for Near-Infrared Diffuse Optical Tomography (NIR-DOT) reconstruction:

### ğŸ—ï¸ Architecture Overview

<div align="center">

```mermaid
graph LR
    A[ğŸ“Š NIR Measurements] --> B[ğŸ§  Transformer<br/>Geometry-Aware]
    B --> C[ğŸ”§ Stage 1 Latent<br/>Space]
    C --> D[ğŸ–¼ï¸ Frozen CNN<br/>Decoder]
    D --> E[ğŸ“ˆ 3D Volume<br/>Reconstruction]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5  
    style C fill:#fff3e0
    style D fill:#e8f5e8
    style E fill:#ffebee
```

</div>

Our hybrid architecture combines the spatial learning capabilities of CNNs with transformer attention mechanisms for geometry-aware reconstruction:

<div align="center">
<img src="figs/architecture.png" alt="Architecture Visualization" width="500"/>
<br/><em>Two-stage architecture: Stage 1 learns spatial priors, Stage 2 maps measurements to latent space</em>
</div>

### âœ¨ Key Features

- ğŸ¯ **Two-Stage Learning**: Decoupled spatial and geometric learning
- ğŸ§  **Transformer Innovation**: Applies transformer attention to DOT inverse problems  
- ğŸ“ **Geometry Awareness**: Source-detector coordinate integration
- ğŸ”„ **Layout Agnostic**: Generalizes across different probe configurations
- ğŸ›ï¸ **Configurable Pipeline**: Easy stage switching and hyperparameter tuning

---

## ğŸ“ Repository Structure

<details>
<summary>ğŸ—‚ï¸ <strong>Explore the codebase structure</strong></summary>

```
mah422/                             # ğŸ  Root directory
â”œâ”€â”€ ğŸ“Š analysis_results/            # Analysis visualizations
â”‚   â””â”€â”€ cross_dataset_analysis_10000_phantoms.png
â”œâ”€â”€ ğŸ’¾ checkpoints/                 # Model checkpoints
â”‚   â”œâ”€â”€ checkpoint_stage1_*.pt
â”‚   â””â”€â”€ checkpoint_stage2_*.pt
â”œâ”€â”€ ğŸ§¬ code/                        # Core implementation (~14k LOC)
â”‚   â”œâ”€â”€ ğŸ“Š data_processing/         # Data generation & loading
â”‚   â”‚   â”œâ”€â”€ data_loader.py         # PyTorch DataLoaders
â”‚   â”‚   â””â”€â”€ data_simulator.py      # Phantom simulation
â”‚   â”œâ”€â”€ ğŸ—ï¸ models/                  # Neural architectures
â”‚   â”‚   â”œâ”€â”€ cnn_autoencoder.py     # 3D CNN autoencoder
â”‚   â”‚   â”œâ”€â”€ transformer_encoder.py # Transformer components
â”‚   â”‚   â”œâ”€â”€ hybrid_model.py        # Combined architecture
â”‚   â”‚   â”œâ”€â”€ global_pooling_encoder.py # Global feature extraction
â”‚   â”‚   â””â”€â”€ spatially_aware_embedding.py  # Geometry processing
â”‚   â”œâ”€â”€ ğŸ¯ training/                # Training pipelines
â”‚   â”‚   â”œâ”€â”€ train_hybrid_model.py  # Main training entry point
â”‚   â”‚   â”œâ”€â”€ stage1_trainer.py      # CNN pretraining
â”‚   â”‚   â”œâ”€â”€ stage2_trainer.py      # Transformer training
â”‚   â”‚   â”œâ”€â”€ training_config.py     # Hyperparameters
â”‚   â”‚   â”œâ”€â”€ training_utils.py      # Training utilities
â”‚   â”‚   â”œâ”€â”€ latent_stats.py        # Latent space analysis
â”‚   â”‚   â””â”€â”€ teacher_stage1.py      # Teacher model wrapper
â”‚   â””â”€â”€ ğŸ”§ utils/                   # Utilities & metrics
â”‚       â”œâ”€â”€ metrics.py             # Evaluation metrics
â”‚       â”œâ”€â”€ standardizers.py       # Data normalization
â”‚       â”œâ”€â”€ viz_recon.py          # Visualization
â”‚       â””â”€â”€ logging_config.py      # Centralized logging
â”œâ”€â”€ ğŸ’¾ data/                        # Generated datasets (HDF5)
â”œâ”€â”€ ğŸ“Š figs/                        # Static figures
â”œâ”€â”€ ğŸ“ logs/                        # Training logs
â”œâ”€â”€ ğŸ”§ nirfaster-FF/               # Forward modeling library
â”œâ”€â”€ âš™ï¸ setup/                       # Environment setup
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ bootstrap_lambdalabs.sh   # Remote setup script
â””â”€â”€ ğŸ“Š wandb/                      # W&B experiment tracking
```

</details>

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Environment Setup

<details>
<summary>ğŸ’» <strong>Local Development</strong></summary>

```bash
# Clone the repository
git clone https://github.com/maxhartml/mah422.git
cd mah422

# Create virtual environment
python3 -m venv env_diss
source env_diss/bin/activate  # Windows: env_diss\Scripts\activate

# Install dependencies
pip install -r setup/requirements.txt
```

</details>

<details>
<summary>â˜ï¸ <strong>Remote Training (Vast.ai/LambdaLabs)</strong></summary>

```bash
# SSH into remote instance
ssh -p <PORT> <USER>@<IP>

# Quick setup
git clone https://github.com/maxhartml/mah422.git && cd mah422
source setup/bootstrap_lambdalabs.sh  # Auto-setup script

# Start persistent session
tmux new -s nir-dot-training
```

</details>

### 2ï¸âƒ£ Data Generation

```bash
# Generate synthetic phantom dataset
python -m code.data_processing.data_simulator

# Expected output structure:
# data/phantom_001/phantom_001_scan.h5  # Measurements + ground truth
# data/phantom_002/phantom_002_scan.h5
# ...
```

### 3ï¸âƒ£ Training Pipeline

<div align="center">

**ğŸ¯ Stage 1: CNN Autoencoder**

</div>

```bash
# Configure Stage 1 in training_config.py
# CURRENT_TRAINING_STAGE = "stage1"

python -m code.training.train_hybrid_model
```

<div align="center">

**ğŸ§  Stage 2: Transformer**

</div>

```bash  
# Configure Stage 2 in training_config.py
# CURRENT_TRAINING_STAGE = "stage2" 

python -m code.training.train_hybrid_model
```

### 4ï¸âƒ£ Results & Monitoring

- ğŸ“Š **Checkpoints**: `checkpoints/checkpoint_stage*_*.pt`
- ğŸ“ **Logs**: `logs/training/` and `logs/data_processing/`
- ğŸ” **W&B Tracking**: Optional project `nir-dot-reconstruction`

---

## ğŸ”§ Configuration & Training

### âš™ï¸ Key Configuration Parameters

<details>
<summary>ğŸ›ï¸ <strong>Training Configuration</strong> (training_config.py)</summary>

```python
# Stage Control
CURRENT_TRAINING_STAGE = "stage1"  # or "stage2"

# Architecture 
LATENT_DIM = 256                    # Latent space dimension
EMBED_DIM = 256                     # Transformer embedding dim
N_MEASUREMENTS = 256                # Subsampled measurements per phantom

# Training
STAGE1_EPOCHS = 150                 # CNN pretraining epochs  
STAGE2_EPOCHS = 100                 # Transformer training epochs
BATCH_SIZE = 4                      # Batch size (memory dependent)

# Optimization
STAGE1_BASE_LR = 1e-4              # CNN base learning rate
STAGE2_BASE_LR = 5e-5              # Transformer base learning rate
USE_EMA = True                     # Exponential moving average
```

</details>

---

## ğŸ”¬ Research & Innovation

### ğŸ“š Scientific Contributions

1. **ğŸ—ï¸ Novel Two-Stage Architecture**: Decoupled spatial and geometric learning for improved generalization
2. **ğŸ§  Transformer for DOT**: Applies transformer attention mechanisms to DOT inverse problems  
3. **ğŸ“ Geometry Integration**: Source-detector coordinate awareness for layout-agnostic reconstruction
4. **ğŸ“Š Comprehensive Evaluation**: Physics-based metrics in raw units with proper statistical reporting

### ğŸ“ Academic Context

<details>
<summary>ğŸ“– <strong>Citation & Publication</strong></summary>

```bibtex
@mastersthesis{hart2025nir_dot_hybrid,
  title={Towards Generalisable Inverse Modelling for Frequency-Domain Diffuse Optical Tomography via a Hybrid CNNâ€“Transformer},
  author={Max Andrew Hart},
  school={University of Birmingham},  
  year={2025},
  type={MSc Dissertation},
  note={AI and Machine Learning}
}
```

</details>

### ğŸ”— References & Acknowledgements

- **NIRFASTer-FF**: Forward modeling framework ([GitHub](https://github.com/milabuob/nirfaster-FF))
- **University of Birmingham**: Computer Science Department
- **Vast.ai**: GPU compute platform for training

#### ğŸ™ Special Acknowledgements

- **Dr. Hamid Dehghani**: Supervisor and research guidance
- **Dr Robin Dale**: Foundational work this research builds upon

---

## ğŸ› ï¸ Development & Contributing

### ğŸ¤ Contributing

We welcome contributions! Please see our contribution guidelines:

1. ğŸ´ Fork the repository
2. ğŸŒŸ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’« Commit changes (`git commit -m 'Add amazing feature'`)
4. ğŸš€ Push to branch (`git push origin feature/amazing-feature`)
5. ğŸ”„ Open a Pull Request

---

## ğŸ“‹ Requirements & Compatibility

### ğŸ’» System Requirements

- **Python**: 3.10+ 
- **GPU**: CUDA-capable GPU (recommended: 16GB+ VRAM)
- **RAM**: 32GB+ for full dataset training
- **Storage**: 50GB+ for datasets and checkpoints

### ğŸ“¦ Dependencies

<details>
<summary>ğŸ” <strong>Key Packages</strong></summary>

```yaml
Core ML/DL:
  - torch: 2.5.1
  - numpy: 1.26.4

Data & Visualization:  
  - h5py: 3.10.0
  - matplotlib: 3.8.2
  - scipy: 1.11.4

ML Tools:
  - scikit-learn: 1.3.2
  - wandb: 0.16.1 (optional)

Utilities:
  - psutil: 5.9.6
```

</details>

---

## ğŸ“ Support & Community

### ğŸ’¬ Get Help

- ğŸ“§ **Email**: [maxhartml@outlook.com](mailto:maxhartml@outlook.com)
- ğŸ› **Issues**: [GitHub Issues](https://github.com/maxhartml/mah422/issues)
- ğŸ’¡ **Discussions**: [GitHub Discussions](https://github.com/maxhartml/mah422/discussions)

### ğŸ·ï¸ Version Information

- **Latest Release**: v1.0.0
- **Total Commits**: 207+ 
- **Main Branch**: `main`
- **License**: MIT

---

<div align="center">

### ğŸ‰ Thank you for your interest in our NIR-DOT research!

**Star â­ this repository if you find it helpful!**

*Made with â¤ï¸ at the University of Birmingham*

---

[![GitHub stars](https://img.shields.io/github/stars/maxhartml/mah422?style=social)](https://github.com/maxhartml/mah422/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/maxhartml/mah422?style=social)](https://github.com/maxhartml/mah422/network/members)

</div>